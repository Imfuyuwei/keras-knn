{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import keras\n",
    "import numpy as np\n",
    "#from keras.applications import resnet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Dense, Reshape\n",
    "from keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Network for Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model which will be used to extract features.\n",
    "# Note that we specify 'include_top=False'. This ensures that we don't load the final\n",
    "# layers specific to the classes the model was originall trained to predict.\n",
    "vgg_model = VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
    "vgg_model.output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features for all images in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = Path(\"~/Desktop/Images/\").expanduser()\n",
    "img_folder_test = Path(\"~/Desktop/images_test/\").expanduser()\n",
    "image_filenames = sorted(glob.glob(str(img_folder / '*.jpg')))\n",
    "len(image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encode_images(encoder, filenames):\n",
    "    batch_size = 16\n",
    "    encoded_dim = np.prod(encoder.output.shape[1:]).value\n",
    "    file_count = len(filenames)\n",
    "    encoded = np.zeros((file_count, encoded_dim))\n",
    "    for start_index in tqdm(list(range(0, file_count, batch_size))):\n",
    "        end_index = min(start_index + batch_size, file_count)\n",
    "        batch_filenames = filenames[start_index:end_index]\n",
    "\n",
    "        batch_images = load_images(batch_filenames)\n",
    "        batch_encoded = encoder.predict(batch_images)\n",
    "        batch_encoded_flat = batch_encoded.reshape((len(batch_encoded), -1))\n",
    "        encoded[start_index:end_index, :] = batch_encoded_flat\n",
    "\n",
    "    return encoded\n",
    "\n",
    "def load_images(filenames):\n",
    "    images = np.zeros((len(filenames), 224, 224, 3))\n",
    "    for i, filename in enumerate(filenames):\n",
    "        img = image.load_img(filename, target_size=(224,224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        images[i, :, :, :] = img_array\n",
    "    return images\n",
    "\n",
    "encoded_imgs = load_encode_images(vgg_model, image_filenames).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the k-NN and Joined Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(encoder, ref_img_coount):\n",
    "    # Query vector (encoding)\n",
    "    flat_dim_size = reduce(lambda x, y: x*y, vgg_model.output_shape[1:])\n",
    "    query_enc_reshaped = Reshape(target_shape=(flat_dim_size,), name='query_enc_flat')(encoder.output)\n",
    "    \n",
    "    # Dot product between query vector and reference vectors\n",
    "    x_a = Dense(units=ref_img_coount, activation='linear', name='dense_1', use_bias=False)(query_enc_reshaped)   \n",
    "                \n",
    "    classifier = Model(inputs=[encoder.input], outputs=x_a)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_model = build_classifier(vgg_model, encoded_imgs.shape[1])\n",
    "joined_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ecnodings(encodings):\n",
    "    ref_norms = np.sqrt(np.square(encodings).sum(axis=0)).reshape((1,encodings.shape[1]))\n",
    "    return encodings / ref_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs_normalized = normalize_ecnodings(encoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Weights to Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_weights = joined_model.get_weights()\n",
    "temp_weights[-1] = encoded_imgs_normalized\n",
    "joined_model.set_weights(temp_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scaled_image(filename, grayscale=False, output_height=0, output_width=0, remove_border=0):\n",
    "    img = kimage.load_img(filename, grayscale=grayscale)\n",
    "    width, height = img.size\n",
    "    img = img.crop((remove_border, remove_border, width - remove_border, height - remove_border))\n",
    "    if output_height > 0 and output_width > 0:\n",
    "        new_size = output_width, output_height\n",
    "        img = img.resize(size=new_size)\n",
    "    img_array = kimage.img_to_array(img)\n",
    "    img_array = img_array.astype('float32') / 255.\n",
    "\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_filename = img_folder_test / \"test_burger_01.jpg\"\n",
    "example_img = image.load_img(example_filename, target_size=(224,224))\n",
    "example_img = image.img_to_array(example_img)\n",
    "example_img = np.expand_dims(example_img, axis=0)\n",
    "example_img = preprocess_input(example_img)\n",
    "prediction = joined_model.predict([example_img]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in prediction.argsort()[-5:][::-1]:\n",
    "    print(image_filenames[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CoreML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
